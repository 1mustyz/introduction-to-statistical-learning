{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1b78fe",
   "metadata": {},
   "source": [
    "# Regression Trees - Chapter 8 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a20a8",
   "metadata": {},
   "source": [
    "## Overview of Regression Trees\n",
    "\n",
    "A **regression tree** is a decision tree model used to predict a continuous outcome (e.g., a baseball player's salary) by dividing the predictor space (the set of all possible values of input features) into distinct, non-overlapping regions. For each region, the model predicts the average response value of the training observations in that region. The tree is built using a series of splitting rules, making it intuitive and easy to interpret compared to other regression models.\n",
    "\n",
    "## Key Example: Predicting Baseball Players' Salaries\n",
    "\n",
    "The book uses the **Hitters dataset** to predict a baseball player's **Salary** (in thousands of dollars, log-transformed for a bell-shaped distribution) based on two predictors:\n",
    "\n",
    "- **Years**: Number of years played in the major leagues\n",
    "- **Hits**: Number of hits made in the previous year\n",
    "\n",
    "### Steps in Building the Regression Tree\n",
    "\n",
    "#### 1. Data Preprocessing\n",
    "- Observations with missing Salary values are removed\n",
    "- Salary is log-transformed to make its distribution more normal (bell-shaped), as raw salary values are often skewed\n",
    "\n",
    "#### 2. Tree Structure (see Figure 8.1 in the book)\n",
    "\n",
    "The tree starts with a top split based on **Years < 4.5**:\n",
    "\n",
    "**Left branch (Years < 4.5)**: Players with less than 4.5 years of experience\n",
    "- Predicted salary: Mean log salary of these players is 5.107\n",
    "- So the predicted salary is $1,000 × e^{5.107} = \\$165,174$\n",
    "\n",
    "**Right branch (Years ≥ 4.5)**: Players with 4.5 or more years of experience\n",
    "- This group is further split based on **Hits < 117.5**:\n",
    "  - **Left sub-branch (Hits < 117.5)**: Predicted salary is $1,000 × e^{5.999} = \\$402,834$\n",
    "  - **Right sub-branch (Hits ≥ 117.5)**: Predicted salary is $1,000 × e^{6.740} = \\$845,346$\n",
    "\n",
    "#### 3. Regions of Predictor Space\n",
    "\n",
    "The tree divides the predictor space (Years and Hits) into three terminal nodes (or leaves):\n",
    "\n",
    "- **R₁**: {Players with Years < 4.5}\n",
    "- **R₂**: {Players with Years ≥ 4.5 and Hits < 117.5}\n",
    "- **R₃**: {Players with Years ≥ 4.5 and Hits ≥ 117.5}\n",
    "\n",
    "These regions are visualized in Figure 8.2 as rectangular partitions in the Years vs. Hits plane.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Years** is the most important predictor, as it determines the first split. Less experienced players (Years < 4.5) earn lower salaries on average.\n",
    "- For more experienced players (Years ≥ 4.5), **Hits** becomes relevant:\n",
    "  - Players with fewer hits (< 117.5) earn less than those with more hits (≥ 117.5)\n",
    "\n",
    "The tree suggests that experience matters most, but among experienced players, performance (hits) further influences salary.\n",
    "\n",
    "### Advantages\n",
    "- The tree is easy to interpret and visualize, unlike complex models like linear regression with interactions or polynomial terms\n",
    "- It captures non-linear relationships and interactions between predictors (e.g., Hits matters only for experienced players)\n",
    "\n",
    "### Limitations\n",
    "- The tree may oversimplify the true relationship between Years, Hits, and Salary, potentially missing nuances that other models (e.g., linear regression) could capture\n",
    "\n",
    "## How Regression Trees Work: General Process\n",
    "\n",
    "The book outlines two main steps for building a regression tree:\n",
    "\n",
    "### Step 1: Divide Predictor Space into Regions\n",
    "\n",
    "- The predictor space (all possible values of predictors X1, X2, ... Xp) is divided into J distinct, non-overlapping regions R1, R2, ... Rj\n",
    "- In the baseball example, the predictors are Years and Hits (p = 2), and the space is divided into three regions (j = 3)\n",
    "- For simplicity, regions are high-dimensional rectangles (or boxes) to make the model interpretable\n",
    "\n",
    "### Step 2: Predict Using Mean Response\n",
    "\n",
    "- For each region Rj, the predicted value for any observation falling in that region is the mean of the response values (e.g., log Salary) for the training observations in region Rj\n",
    "- Example: In region R1 (Years < 4.5), the mean log salary is 5.107, so the prediction is $1,000 × e^{5.107}$\n",
    "\n",
    "## Building the Tree: Recursive Binary Splitting\n",
    "\n",
    "To create the regions, regression trees use a **top-down, greedy approach** called **recursive binary splitting**:\n",
    "\n",
    "### Top-Down\n",
    "- Start with all observations in a single region (the root of the tree)\n",
    "- Successively split the predictor space into two new regions (branches) at each step\n",
    "\n",
    "### Greedy\n",
    "- At each step, choose the split that minimizes the **Residual Sum of Squares (RSS)** at that moment, without considering future splits\n",
    "- RSS is defined as:\n",
    "\n",
    "$$\\sum_{j=1}^J \\sum_{i \\in R_j} (y_i - \\hat{y}_{R_j})^2$$\n",
    "\n",
    "where $\\hat{y}_{R_j}$ is the mean response in region $R_j$, and $y_i$ is the observed response for observation $i$.\n",
    "\n",
    "### Splitting Process\n",
    "\n",
    "1. For each predictor Xj (e.g., Years or Hits) and possible cutpoint s, consider splitting the space into two half-planes:\n",
    "   - R1(j, s) = {X|Xj < s}\n",
    "   - R2(j, s) = {X|Xj ≥ s}\n",
    "\n",
    "2. Choose the predictor j and cutpoint s that minimize:\n",
    "\n",
    "```\n",
    "∑(yi − ŷR1)² + ∑(yi − ŷR2)²\n",
    "i: xi ∈R1(j,s)    i: xi ∈R2(j,s)\n",
    "```\n",
    "\n",
    "where ŷR1 and ŷR2 are the mean responses in the two regions.\n",
    "\n",
    "3. In the baseball example, the first split (Years < 4.5) was chosen because it reduced RSS the most. The second split (Hits < 117.5) further minimized RSS within the right branch (Years ≥ 4.5).\n",
    "\n",
    "### Continue Splitting\n",
    "- Repeat the process on one of the resulting regions, creating new branches, until a stopping criterion is met (e.g., no region has more than 5 observations)\n",
    "\n",
    "### Prediction\n",
    "- For a new observation, determine which region it falls into based on its predictor values (Years and Hits)\n",
    "- Predict the mean response of the training observations in that region\n",
    "\n",
    "## Key Terminology\n",
    "\n",
    "- **Terminal Nodes (Leaves)**: The final regions (e.g., R1, R2, R3) where predictions are made\n",
    "- **Internal Nodes**: Points where splits occur (e.g., Years < 4.5, Hits < 117.5)\n",
    "- **Branches**: The segments connecting nodes in the tree\n",
    "- **Recursive Binary Splitting**: The process of repeatedly splitting regions into two sub-regions to minimize RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c8a36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45b316bc",
   "metadata": {},
   "source": [
    "## What's Happening Here?\n",
    "\n",
    "In the regression tree for the Hitters dataset, the predictor space (based on Years and Hits) is divided into regions. For players in the region defined by Years < 4.5 (less than 4.5 years of experience), the regression tree predicts their salary using the mean log salary of all training observations in that region. The mean log salary is given as 5.107, and the final predicted salary is calculated as:\n",
    "\n",
    "$1,000 × e^{5.107} = \\$165,174$\n",
    "\n",
    "Here's why this calculation is done and what it means:\n",
    "\n",
    "## Step-by-Step Explanation\n",
    "\n",
    "### 1. Why Log Salary?\n",
    "\n",
    "- The Salary variable in the Hitters dataset represents players' salaries in thousands of dollars. However, raw salary values are often skewed (e.g., a few players earn extremely high salaries, while most earn less).\n",
    "- To make the distribution more normal (bell-shaped), the salaries are log-transformed using the natural logarithm (base e). This is a common technique in regression to stabilize variance and handle skewed data.\n",
    "- So, instead of modeling the raw salary (e.g., $165,174), the model works with log(Salary), where Salary is in thousands of dollars.\n",
    "\n",
    "### 2. Mean Log Salary = 5.107\n",
    "\n",
    "- For players in the region Years < 4.5, the regression tree calculates the average of the log-transformed salaries (log(Salary)) for all training observations in that region.\n",
    "- The book states this average is 5.107. This means:\n",
    "\n",
    "Mean log(Salary) = 5.107\n",
    "\n",
    "where Salary is in thousands of dollars.\n",
    "\n",
    "### 3. Converting Back to Dollars: e^5.107\n",
    "\n",
    "- Since the model works with log-transformed salaries, the mean log salary (5.107) is in the log scale. To get the predicted salary in dollars, we need to \"undo\" the logarithm by exponentiating using the base e (since the natural logarithm was used).\n",
    "- The exponential function e^x is the inverse of the natural logarithm. So:\n",
    "\n",
    "e^5.107\n",
    "\n",
    "gives the predicted salary in thousands of dollars.\n",
    "\n",
    "### 4. Multiplying by 1000\n",
    "\n",
    "- The Salary variable in the dataset is recorded in thousands of dollars (e.g., a Salary value of 165 means $165,000).\n",
    "- After computing e^5.107, the result is in thousands of dollars. To convert it to actual dollars, multiply by 1000:\n",
    "\n",
    "$1,000 × e^{5.107}$\n",
    "\n",
    "### 5. Calculating the Value\n",
    "\n",
    "- Compute e^5.107:\n",
    "\n",
    "e^5.107 ≈ 165.174\n",
    "\n",
    "This means the predicted salary is approximately 165.174 thousand dollars.\n",
    "\n",
    "- Multiply by 1000 to get the dollar amount:\n",
    "\n",
    "165.174 × 1000 = $165,174\n",
    "\n",
    "- So, for any player with Years < 4.5, the regression tree predicts a salary of $165,174."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0879d8dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7905cf49",
   "metadata": {},
   "source": [
    "## What is Tree Pruning and Why Do We Need It?\n",
    "\n",
    "When building a regression tree (like the one predicting baseball players' salaries based on Years and Hits), the process of recursive binary splitting can create a very large and complex tree (denoted $T_0$) that fits the training data extremely well. However, this often leads to **overfitting**, where the model captures noise in the training data and performs poorly on new, unseen data (the test set). \n",
    "\n",
    "**Pruning** is the process of simplifying a large tree by trimming back branches to create a smaller subtree that balances:\n",
    "- Fit to the training data (low bias)\n",
    "- Model simplicity (low variance, better generalization to test data)\n",
    "\n",
    "Pruning reduces the number of terminal nodes (leaves) to avoid overly complex trees, making the model more interpretable and improving its performance on test data.\n",
    "\n",
    "### Key Problem with Unpruned Trees\n",
    "\n",
    "The book explains that a large tree (grown using recursive binary splitting until, say, each region has very few observations) may have low training error but high test error due to overfitting. For example, in the Hitters dataset, a tree with many splits might perfectly capture the training salaries but fail to generalize to new players' salaries.\n",
    "\n",
    "An alternative to growing a smaller tree is to set a high threshold for the reduction in Residual Sum of Squares (RSS) for each split. However, this is short-sighted because a split that seems unimportant early on (small RSS reduction) might lead to a highly effective split later. For instance, splitting on Years might not reduce RSS much initially but could enable a critical split on Hits later.\n",
    "\n",
    "### Solution: Cost Complexity Pruning (Weakest Link Pruning)\n",
    "\n",
    "Instead of limiting splits during tree growth, the book proposes:\n",
    "1. Growing a large tree ($T_0$) using recursive binary splitting, stopping only when each terminal node has a minimum number of observations (e.g., 5)\n",
    "2. Pruning this tree back to find a simpler subtree that minimizes test error\n",
    "\n",
    "Cost complexity pruning (also called **weakest link pruning**) achieves this by considering a sequence of subtrees, each associated with a tuning parameter $\\alpha$. The goal is to find the subtree that minimizes a cost complexity criterion:\n",
    "\n",
    "$$\\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha |T|$$\n",
    "\n",
    "Where:\n",
    "- $|T|$: Number of terminal nodes (leaves) in the subtree $T$\n",
    "- $R_m$: The $m$-th terminal node (a rectangular region in the predictor space, e.g., Years < 4.5)\n",
    "- $\\hat{y}_{R_m}$: Mean response (e.g., log salary) of training observations in region $R_m$\n",
    "- $\\sum_{i: x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2$: RSS for region $R_m$, summing the squared differences between observed responses ($y_i$, e.g., log salary) and the predicted mean ($\\hat{y}_{R_m}$)\n",
    "- $\\alpha |T|$: A penalty term that increases with the number of terminal nodes, controlled by the tuning parameter $\\alpha$\n",
    "\n",
    "### Understanding the Cost Complexity Criterion\n",
    "\n",
    "The formula balances two goals:\n",
    "\n",
    "1. **Minimize RSS**: The first term, $\\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} (y_i - \\hat{y}_{R_m})^2$, measures how well the tree fits the training data (lower RSS = better fit)\n",
    "\n",
    "2. **Penalize Complexity**: The term $\\alpha |T|$ penalizes trees with many terminal nodes. A larger $\\alpha$ favors smaller trees (fewer leaves), while $\\alpha = 0$ results in the full tree $T_0$\n",
    "\n",
    "- When $\\alpha = 0$: No penalty for complexity, so the subtree is the full tree $T_0$, which minimizes training RSS but may overfit\n",
    "- As $\\alpha$ increases: The penalty for additional leaves grows, favoring smaller subtrees with fewer terminal nodes, reducing variance at the cost of slightly higher bias\n",
    "\n",
    "This is similar to the lasso (Chapter 6), where a penalty term controls model complexity by shrinking coefficients. Here, $\\alpha$ controls tree size.\n",
    "\n",
    "### How Pruning Works\n",
    "\n",
    "1. **Grow a Large Tree**: Start with a complex tree $T_0$ using recursive binary splitting\n",
    "2. **Prune Sequentially**: As $\\alpha$ increases, branches are pruned in a nested way. The algorithm identifies the \"weakest\" branches (those contributing the least to RSS reduction relative to their complexity) and removes them, creating a sequence of smaller subtrees\n",
    "3. **Select the Best Subtree**: Use cross-validation or a validation set to estimate the test error for each subtree (corresponding to different $\\alpha$ values). Choose the $\\alpha$ that minimizes the estimated test error\n",
    "4. **Apply the Subtree**: Return to the full dataset and use the selected $\\alpha$ to obtain the final pruned subtree\n",
    "\n",
    "This process is efficient because it doesn't require evaluating every possible subtree—just a sequence determined by $\\alpha$.\n",
    "\n",
    "### Algorithm 8.1: Building a Regression Tree with Pruning\n",
    "\n",
    "The book summarizes the process in Algorithm 8.1:\n",
    "\n",
    "1. **Grow a Large Tree**: Use recursive binary splitting on the training data, stopping when each terminal node has fewer than a minimum number of observations (e.g., 5)\n",
    "\n",
    "2. **Apply Cost Complexity Pruning**: Generate a sequence of subtrees by varying $\\alpha$, where each subtree minimizes the cost complexity criterion\n",
    "\n",
    "3. **Choose $\\alpha$ via Cross-Validation**:\n",
    "   - Divide the training data into $K$ folds (e.g., $K = 6$)\n",
    "   - For each fold $k$:\n",
    "     - Build a tree and prune it using all but the $k$-th fold\n",
    "     - Compute the mean squared prediction error (MSE) on the $k$-th fold for each $\\alpha$\n",
    "   - Average the MSE across all folds for each $\\alpha$\n",
    "   - Pick the $\\alpha$ that minimizes the average cross-validated MSE\n",
    "\n",
    "4. **Return the Final Subtree**: Use the chosen $\\alpha$ to select the corresponding subtree from the full dataset\n",
    "\n",
    "### Application to the Hitters Dataset\n",
    "\n",
    "The book applies this process to the Hitters dataset to predict log(Salary) using nine features (not just Years and Hits, unlike the earlier example). Here's how it works:\n",
    "\n",
    "- **Data Split**: The dataset is split into 132 training observations and 131 test observations\n",
    "- **Grow a Large Tree**: A complex regression tree is built on the training data using recursive binary splitting\n",
    "- **Pruning**: Cost complexity pruning is applied, creating a sequence of subtrees with varying numbers of terminal nodes by adjusting $\\alpha$\n",
    "- **Cross-Validation**: Six-fold cross-validation is used (since 132 is divisible by 6) to estimate the mean squared error (MSE) for each subtree (i.e., for each $\\alpha$). The MSE is calculated on the held-out fold for each $\\alpha$\n",
    "\n",
    "#### Results (Figures 8.4 and 8.5):\n",
    "- **Figure 8.4**: Shows the unpruned tree (likely complex with many leaves)\n",
    "- **Figure 8.5**: Plots the cross-validated MSE (green curve) and test MSE (orange curve) as a function of the number of leaves, with standard error bars. The training error (black curve) is also shown\n",
    "  - The cross-validated MSE is minimized for a tree with three terminal nodes, suggesting this is the optimal subtree size\n",
    "  - The test MSE also dips at three nodes, though it's lowest at ten nodes, indicating the cross-validated estimate is a good approximation of test error\n",
    "- **Final Tree**: The pruned tree with three terminal nodes is shown in Figure 8.1 (the earlier example with splits on Years < 4.5 and Hits < 117.5)\n",
    "\n",
    "### Intuition Behind Figures 8.4 and 8.5\n",
    "\n",
    "- **Figure 8.4**: The unpruned tree is likely large and complex, with many splits, fitting the training data closely but potentially overfitting\n",
    "\n",
    "- **Figure 8.5**:\n",
    "  - The green curve (cross-validated MSE) shows how the estimated test error changes with the number of leaves. It's lowest at three leaves, indicating this subtree generalizes best\n",
    "  - The orange curve (test MSE) confirms that a three-node tree performs well, though a ten-node tree is slightly better (but cross-validation picks three for simplicity and robustness)\n",
    "  - The black curve (training error) decreases as the tree grows larger (more leaves), but this comes at the cost of overfitting, as seen by the higher test error for larger trees\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "#### Why Prune?\n",
    "Pruning prevents overfitting by simplifying a complex tree, improving test set performance and interpretability.\n",
    "\n",
    "#### Cost Complexity Pruning:\n",
    "- Uses a tuning parameter $\\alpha$ to balance fit (RSS) and complexity (number of leaves)\n",
    "- Larger $\\alpha$ favors smaller trees; $\\alpha = 0$ keeps the full tree\n",
    "\n",
    "#### Cross-Validation:\n",
    "- Estimates test error for different subtrees to select the optimal $\\alpha$\n",
    "- In the Hitters example, a three-node tree was chosen as it minimized cross-validated MSE\n",
    "\n",
    "#### Hitters Example:\n",
    "A large tree was grown, then pruned to a simpler tree with three leaves (Years < 4.5, Hits < 117.5 splits), which balances fit and generalization.\n",
    "\n",
    "#### Analogy to Lasso:\n",
    "The penalty term $\\alpha |T|$ is like the lasso's penalty, controlling model complexity to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061fbcf0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9b6059e",
   "metadata": {},
   "source": [
    "## What is a Classification Tree?\n",
    "\n",
    "A classification tree is a decision tree used to predict a qualitative (categorical) response (e.g., Yes/No, Red/Blue/Green) rather than a quantitative one (like salary in a regression tree). It's similar to a regression tree in structure but differs in how predictions are made and how splits are evaluated.\n",
    "\n",
    "- **Prediction**: For a given region (terminal node), the predicted class is the most common class among the training observations in that region.\n",
    "- **Interpretation**: Beyond the predicted class, we're often interested in the class proportions in each region (e.g., what percentage of observations in a region belong to each class), as this indicates the confidence of the prediction.\n",
    "\n",
    "### How Classification Trees Work\n",
    "\n",
    "The process of building a classification tree is similar to a regression tree, using recursive binary splitting to divide the predictor space into regions. However, since the response is categorical, the Residual Sum of Squares (RSS) used in regression trees isn't applicable. Instead, we need metrics to evaluate the quality of splits based on class purity or error.\n",
    "\n",
    "### Splitting Criteria\n",
    "\n",
    "The book introduces three metrics to assess splits in a classification tree:\n",
    "\n",
    "#### Classification Error Rate:\n",
    "- **Definition**: The fraction of training observations in a region that do not belong to the most common class.\n",
    "- **Formula**: \n",
    "  $$E = 1 - \\max_k (\\hat{p}_{mk})$$\n",
    "  where $\\hat{p}_{mk}$ is the proportion of training observations in the $m$-th region (terminal node) that belong to class $k$.\n",
    "- **Intuition**: Measures the misclassification rate in a region. If 80% of observations in a region are Class A and 20% are Class B, the error rate is $1 - 0.8 = 0.2$ (20% misclassified).\n",
    "- **Limitation**: The classification error rate is not sensitive enough for tree-growing because it doesn't strongly penalize impure nodes (where classes are mixed).\n",
    "\n",
    "#### Gini Index:\n",
    "- **Definition**: A measure of node impurity (total variance across classes).\n",
    "- **Formula**: \n",
    "  $$G = \\sum_{k=1}^K \\hat{p}_{mk} (1 - \\hat{p}_{mk})$$\n",
    "  where $K$ is the number of classes, and $\\hat{p}_{mk}$ is the proportion of class $k$ in region $m$.\n",
    "- **Intuition**: The Gini index is small when the $\\hat{p}_{mk}$ values are close to 0 or 1 (i.e., most observations in a region belong to one class, making the node \"pure\"). For example:\n",
    "  - If a region is 100% Class A ($\\hat{p}_{mA} = 1, \\hat{p}_{mB} = 0$), then $G = 1 \\cdot (1-1) + 0 \\cdot (1-0) = 0$ (perfectly pure).\n",
    "  - If a region is 50% Class A, 50% Class B ($\\hat{p}_{mA} = 0.5, \\hat{p}_{mB} = 0.5$), then $G = 0.5 \\cdot 0.5 + 0.5 \\cdot 0.5 = 0.5$ (highly impure).\n",
    "- **Use**: Preferred for splitting because it's sensitive to node purity, encouraging splits that create regions dominated by one class.\n",
    "\n",
    "#### Entropy (Cross-Entropy or Deviance):\n",
    "- **Definition**: Another measure of node impurity.\n",
    "- **Formula**: \n",
    "  $$D = -\\sum_{k=1}^K \\hat{p}_{mk} \\log \\hat{p}_{mk}$$\n",
    "  where $\\log$ is the natural logarithm, and $\\hat{p}_{mk} \\geq 0$. Note: When $\\hat{p}_{mk} = 0$, the term $\\hat{p}_{mk} \\log \\hat{p}_{mk}$ is defined as 0 (by convention, as the limit approaches 0).\n",
    "- **Intuition**: Entropy is small when $\\hat{p}_{mk}$ values are close to 0 or 1 (pure nodes). For example:\n",
    "  - If a region is 100% Class A ($\\hat{p}_{mA} = 1, \\hat{p}_{mB} = 0$), then $D = -[1 \\cdot \\log 1 + 0 \\cdot \\log 0] = 0$ (pure).\n",
    "  - If a region is 50% Class A, 50% Class B, then $D = -[0.5 \\cdot \\log 0.5 + 0.5 \\cdot \\log 0.5] \\approx 0.693$ (impure).\n",
    "- **Use**: Like the Gini index, entropy is sensitive to node purity and is often used for splitting.\n",
    "\n",
    "### Why Gini or Entropy Over Classification Error?\n",
    "\n",
    "- **Sensitivity to Purity**: Gini and entropy penalize mixed regions more heavily than the classification error rate, making them better for growing trees. A split that slightly improves class separation (e.g., from 50/50 to 60/40) may not reduce the error rate much but will reduce Gini or entropy, encouraging purer nodes.\n",
    "- **Pruning**: Any of the three metrics can be used for pruning, but the classification error rate is preferred if the goal is to maximize prediction accuracy of the final tree, as it directly measures misclassification.\n",
    "\n",
    "### Building a Classification Tree\n",
    "\n",
    "#### Recursive Binary Splitting:\n",
    "- Similar to regression trees, the predictor space is divided into regions using binary splits (e.g., Age < 50 or Sex = Male).\n",
    "- At each step, choose the predictor and cutpoint that minimize the Gini index or entropy (rather than RSS).\n",
    "- Continue splitting until a stopping criterion (e.g., minimum number of observations per node).\n",
    "\n",
    "#### Prediction:\n",
    "- For a new observation, determine which region it falls into.\n",
    "- Predict the most common class in that region.\n",
    "- Optionally, report the class proportions (e.g., 80% Yes, 20% No) to indicate prediction confidence.\n",
    "\n",
    "#### Pruning:\n",
    "- Grow a large tree and prune it back using cost complexity pruning, similar to regression trees, but using Gini, entropy, or classification error as the criterion.\n",
    "- The cost complexity criterion is:\n",
    "  $$\\sum_{m=1}^{|T|} \\sum_{i: x_i \\in R_m} \\text{Error}(y_i, \\hat{y}_{R_m}) + \\alpha |T|$$\n",
    "  where the error term is typically Gini or entropy for splitting, or classification error for pruning, and $\\alpha$ controls tree size.\n",
    "\n",
    "### Example: Heart Dataset\n",
    "\n",
    "The book illustrates classification trees using the Heart dataset, which predicts a binary outcome (HD = Yes/No, indicating heart disease based on an angiographic test) for 303 patients based on 13 predictors, including:\n",
    "- **Quantitative**: Age, Cholesterol (Chol), etc.\n",
    "- **Qualitative**: Sex, Thal (Thallium stress test), ChestPain, etc.\n",
    "\n",
    "#### Key Details:\n",
    "- **Tree Structure**: Cross-validation results in a pruned tree with six terminal nodes.\n",
    "- **Splits on Qualitative Predictors**: Splits on qualitative variables (e.g., Thal, ChestPain) assign some categories to one branch and others to the other.\n",
    "  - Example: The top split on Thal sends observations with Thal = normal to the left branch and Thal = fixed or reversible defects to the right.\n",
    "  - Another split on ChestPain:bc means the left branch includes observations with ChestPain = typical angina or non-anginal pain (second and third values), while the right branch includes the others (atypical angina, asymptomatic).\n",
    "\n",
    "#### Surprising Feature: Splits with Same Predicted Class:\n",
    "In Figure 8.6, some splits (e.g., RestECG < 1) result in two terminal nodes that both predict Yes for heart disease. Why split if the prediction doesn't change?\n",
    "\n",
    "**Reason**: The split improves node purity. For example:\n",
    "- Right leaf (RestECG ≥ 1): All 9 observations are Yes (100% pure, Gini = 0).\n",
    "- Left leaf (RestECG < 1): 7 out of 11 observations are Yes (less pure, Gini > 0).\n",
    "\n",
    "**Why Purity Matters**: A purer node (e.g., 100% Yes) gives higher confidence in predictions for test observations. For the less pure node (7/11 Yes), the prediction is still Yes, but with lower confidence (class proportions: ~64% Yes, ~36% No).\n",
    "\n",
    "**Metrics**: The split reduces the Gini index or entropy (indicating purer nodes) even if the classification error rate doesn't change, as Gini and entropy are more sensitive to these improvements.\n",
    "\n",
    "#### Outcome:\n",
    "- The tree with six terminal nodes balances accuracy and simplicity, as determined by cross-validation.\n",
    "- For a new patient, the tree assigns them to a region based on their predictors (e.g., Age, Thal, ChestPain) and predicts Yes or No for heart disease, along with class proportions for confidence.\n",
    "\n",
    "### Key Differences from Regression Trees\n",
    "\n",
    "| Aspect | Regression Trees | Classification Trees |\n",
    "|--------|------------------|---------------------|\n",
    "| **Response** | Continuous value (mean) | Categorical class |\n",
    "| **Splitting Criterion** | Minimize RSS | Minimize Gini index or entropy (preferred) or classification error (less sensitive) |\n",
    "| **Prediction** | Mean of the region's responses | Most common class in the region, with class proportions for interpretation |\n",
    "| **Qualitative Predictors** | Both handle qualitative predictors | Assign categories to branches (e.g., Thal = normal vs. others) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5edd9e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12e3ad67",
   "metadata": {},
   "source": [
    "## Section 8.1.3: Trees Versus Linear Models\n",
    "\n",
    "This section compares decision trees (both regression and classification) with classical linear models (e.g., linear regression for continuous outcomes, logistic regression for binary outcomes) as introduced in Chapters 3 and 4. The comparison focuses on their modeling assumptions and performance.\n",
    "\n",
    "### Modeling Assumptions\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "**Model Form:**\n",
    "```\n",
    "f(X) = β₀ + ∑ⱼ₌₁ᵖ Xⱼβⱼ\n",
    "```\n",
    "\n",
    "- Predicts the response as a linear combination of predictors X₁, X₂, …, Xₚ, with coefficients β₀, β₁, …, βₚ.\n",
    "\n",
    "**Example:** For the Hitters dataset, a linear regression model might predict log(Salary) as:\n",
    "```\n",
    "log(Salary) = β₀ + β₁ · Years + β₂ · Hits\n",
    "```\n",
    "\n",
    "- Assumes a linear relationship between predictors and the response, with additive effects.\n",
    "- **Strength:** Works well when the relationship is approximately linear or can be made linear (e.g., via transformations).\n",
    "\n",
    "#### Regression Trees\n",
    "\n",
    "**Model Form:**\n",
    "```\n",
    "f(X) = ∑ᵐ₌₁ᴹ cₘ · 1(X ∈ Rₘ)\n",
    "```\n",
    "\n",
    "- Divides the predictor space into M non-overlapping regions R₁, R₂, …, Rₘ (e.g., as shown in Figure 8.3 for the Hitters dataset).\n",
    "- Predicts a constant value cₘ (the mean response in region Rₘ) for all observations in that region.\n",
    "\n",
    "**Example:** For the Hitters dataset, the tree predicts log(Salary) = 5.107 (e^5.107 × 1000 = $165,174) for players with Years < 4.5.\n",
    "\n",
    "- **Key Feature:** Models non-linear relationships by partitioning the predictor space into rectangular regions, with no assumption of linearity.\n",
    "\n",
    "#### Classification Trees\n",
    "\n",
    "- Similar to regression trees, but predict the most common class in each region (e.g., Yes/No for heart disease in the Heart dataset).\n",
    "\n",
    "**Example:** In the Heart dataset, a region might predict \"Yes\" for heart disease if most training observations in that region (e.g., Thal = normal, Age > 50) are Yes.\n",
    "\n",
    "### Which Model is Better?\n",
    "\n",
    "**Depends on the Data:**\n",
    "\n",
    "#### Linear Models Excel When:\n",
    "- The relationship between predictors and response is approximately linear or can be approximated with transformations (e.g., log(Salary) vs. Years might be roughly linear in the Hitters dataset).\n",
    "- Linear models leverage this structure for better performance.\n",
    "\n",
    "#### Trees Excel When:\n",
    "- The relationship is highly non-linear or complex, with interactions that are hard to model linearly.\n",
    "\n",
    "**For example:**\n",
    "- In the Heart dataset, the effect of Thal or ChestPain on heart disease might depend on other predictors in a non-linear way, which trees capture naturally by splitting on different variables.\n",
    "- Trees create regions (e.g., Years < 4.5, Hits ≥ 117.5) that model interactions without needing explicit interaction terms.\n",
    "\n",
    "**Figure 8.7:** This figure (not provided but described) likely shows a dataset where a linear model fails to capture a non-linear pattern (e.g., a curved relationship), while a tree's piecewise constant regions fit it better, or vice versa.\n",
    "\n",
    "### Evaluating Performance\n",
    "\n",
    "- Use cross-validation or a validation set (Chapter 5) to estimate test error (e.g., mean squared error for regression, misclassification rate for classification).\n",
    "- For the Hitters dataset, if log(Salary) is roughly linear in Years and Hits, linear regression might have lower test error. If the relationship is complex (e.g., salary jumps significantly after a certain number of years), a tree might perform better.\n",
    "- For the Heart dataset, if predictors like Age and Thal interact non-linearly to predict heart disease, a classification tree might outperform logistic regression.\n",
    "\n",
    "### Other Considerations\n",
    "\n",
    "- **Interpretability:** Trees are often preferred for their intuitive, visual representation (e.g., Figure 8.1 for Hitters shows clear splits on Years and Hits).\n",
    "- **Complexity:** Linear models require specifying interactions or transformations; trees automatically capture these through splits.\n",
    "\n",
    "---\n",
    "\n",
    "## Section 8.1.4: Advantages and Disadvantages of Trees\n",
    "\n",
    "This section outlines the pros and cons of decision trees compared to classical approaches like linear regression and logistic regression.\n",
    "\n",
    "### Advantages of Decision Trees\n",
    "\n",
    "#### Easy to Explain\n",
    "- Trees are intuitive, even for non-experts. For example, in the Hitters dataset, the rule \"If Years < 4.5, predict salary = $165,174\" is easier to understand than a linear regression equation with coefficients.\n",
    "- Trees mimic human decision-making (e.g., \"If a patient has Thal = normal, check Age next\" in the Heart dataset).\n",
    "\n",
    "#### Graphical Representation\n",
    "- Trees can be visualized (e.g., Figure 8.1 for Hitters, Figure 8.6 for Heart), making them easy to interpret, especially for small trees.\n",
    "\n",
    "**Example:** The Heart dataset tree shows clear splits (e.g., Thal, RestECG), helping doctors understand decision rules for heart disease.\n",
    "\n",
    "#### Handle Qualitative Predictors Naturally\n",
    "- Trees can split on categorical variables (e.g., Thal = normal vs. fixed/reversible in the Heart dataset) without needing dummy variables, unlike linear models.\n",
    "\n",
    "**Example:** The ChestPain:bc split assigns \"typical angina\" and \"non-anginal pain\" to one branch, simplifying the modeling process.\n",
    "\n",
    "### Disadvantages of Decision Trees\n",
    "\n",
    "#### Lower Predictive Accuracy\n",
    "- Trees often have higher test error than other methods (e.g., linear regression, logistic regression, or more advanced techniques like SVMs).\n",
    "\n",
    "**Examples:**\n",
    "- In the Hitters dataset, a linear regression model might capture a smooth relationship between Years, Hits, and log(Salary) better than a tree's piecewise constant predictions.\n",
    "- In the Heart dataset, logistic regression might model the probability of heart disease more precisely than a tree's discrete class predictions.\n",
    "\n",
    "#### Non-Robustness\n",
    "- Trees are sensitive to small changes in the data. A slight change in the training set (e.g., adding/removing a few observations) can lead to a completely different tree structure.\n",
    "\n",
    "**Example:** In the Hitters dataset, a different sample might change the split from Years < 4.5 to Years < 5, altering the entire tree.\n",
    "\n",
    "### Improving Trees\n",
    "\n",
    "The book hints that aggregating multiple trees using methods like bagging, random forests, and boosting (covered in Section 8.2) can significantly improve predictive performance.\n",
    "\n",
    "These methods combine many trees to reduce variance (addressing non-robustness) and improve accuracy, making tree-based methods competitive with other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bf026a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef83982b",
   "metadata": {},
   "source": [
    "## Overview of Ensemble Methods\n",
    "\n",
    "An ensemble method combines multiple simple models, called weak learners, to create a stronger, more accurate model. In this context, the weak learners are decision trees (regression or classification trees from Section 8.1). Bagging, random forests, boosting, and Bayesian additive regression trees are ensemble methods that improve the performance of individual trees, which often suffer from high variance and limited predictive accuracy. This section focuses on bagging.\n",
    "\n",
    "## Section 8.2.1: Bagging\n",
    "\n",
    "### What is Bagging?\n",
    "\n",
    "Bagging (short for Bootstrap Aggregation) is a general-purpose technique to reduce the variance of a statistical learning method, particularly decision trees. Decision trees have high variance, meaning that small changes in the training data can lead to very different trees (as noted in Section 8.1.4). Bagging addresses this by averaging predictions from multiple trees built on different subsets of the data, resulting in a more stable and accurate model.\n",
    "\n",
    "### Why Do Trees Have High Variance?\n",
    "\n",
    "As discussed in Section 8.1, a single decision tree (e.g., the Hitters regression tree or Heart classification tree) can overfit the training data, especially if grown deep without pruning.\n",
    "\n",
    "- If you split the training data into two random halves and fit a tree to each, the resulting trees might have different splits (e.g., Years < 4.5 vs. Years < 5 for Hitters), leading to different predictions. This instability is high variance.\n",
    "- In contrast, linear regression (Section 8.1.3) has lower variance when the sample size (n) is large relative to the number of predictors (p), as it assumes a stable linear structure.\n",
    "\n",
    "### How Bagging Works\n",
    "\n",
    "Bagging reduces variance by averaging predictions from multiple trees, each trained on a different bootstrap sample of the training data. Here's the process:\n",
    "\n",
    "#### Bootstrap Sampling:\n",
    "- The bootstrap (introduced in Chapter 5) involves taking repeated random samples with replacement from the training dataset.\n",
    "- Given a dataset with (n) observations, a bootstrap sample is created by sampling (n) observations with replacement. Some observations may appear multiple times, while others may be left out.\n",
    "- Generate (B) bootstrap samples (e.g., B = 100).\n",
    "\n",
    "#### Train Trees:\n",
    "- For each of the (B) bootstrap samples, build a decision tree (regression or classification) without pruning. These trees are grown deep, so each has high variance but low bias (they fit their bootstrap sample closely).\n",
    "- Let f̂*b(x) be the prediction for input (x) from the (b)-th tree (trained on the (b)-th bootstrap sample).\n",
    "\n",
    "#### Aggregate Predictions:\n",
    "\n",
    "##### For Regression:\n",
    "Average the predictions from all (B) trees:\n",
    "```\n",
    "f̂_bag(x) = (1/B) ∑ᵦ₌₁ᴮ f̂*b(x)\n",
    "```\n",
    "\n",
    "**Example:** For the Hitters dataset, predict log(Salary) for a player by averaging the log(Salary) predictions from (B) regression trees.\n",
    "\n",
    "##### For Classification:\n",
    "Take a majority vote across the (B) trees. The predicted class is the one most frequently predicted by the trees.\n",
    "\n",
    "**Example:** For the Heart dataset, if 60 out of 100 trees predict \"Yes\" for heart disease, the bagged prediction is \"Yes.\"\n",
    "\n",
    "### Why It Works:\n",
    "- Each tree has high variance (unstable predictions) but low bias (fits its bootstrap sample well).\n",
    "- Averaging reduces variance, similar to how the variance of the mean of (n) independent observations Z₁, …, Zₙ (each with variance σ²) is σ²/n.\n",
    "- By combining (B) trees (e.g., hundreds or thousands), bagging produces a model with lower variance and better test set accuracy.\n",
    "\n",
    "### Practical Considerations:\n",
    "- **Number of Trees (B):** The book notes that (B) (e.g., 100) is not critical, as using a large (B) doesn't lead to overfitting. Choose a (B) large enough for the test error to stabilize (see Figure 8.8).\n",
    "- **Deep Trees:** Bagged trees are grown deep without pruning to maintain low bias, relying on averaging to reduce variance.\n",
    "\n",
    "### Application to the Heart Dataset\n",
    "\n",
    "**Context:** Bagging is applied to the Heart dataset (Section 8.1.2) to predict heart disease (Yes/No) using 13 predictors (e.g., Age, Thal, ChestPain).\n",
    "\n",
    "**Figure 8.8:**\n",
    "- Shows the test error rate (black and orange curves) as a function of (B), the number of bootstrapped trees.\n",
    "- The test error for bagging is slightly lower than that of a single classification tree (dashed line), showing improvement.\n",
    "- The OOB error (green and blue curves) is lower than the test error \"by chance\" in this case, indicating good performance.\n",
    "\n",
    "**Result:** Bagging with B = 100 trees achieves good performance, as the error stabilizes, and more trees don't significantly improve results.\n",
    "\n",
    "## Out-of-Bag (OOB) Error Estimation\n",
    "\n",
    "Bagging provides a convenient way to estimate test error without cross-validation or a separate validation set, using out-of-bag (OOB) observations.\n",
    "\n",
    "### What are OOB Observations?\n",
    "- In each bootstrap sample, about two-thirds of the (n) observations are included (on average), and the remaining one-third are left out (not used to train that tree). These are the OOB observations for that tree.\n",
    "- For each observation (i), there are approximately B/3 trees where it is OOB (not used in training).\n",
    "\n",
    "### OOB Prediction:\n",
    "For the (i)-th observation:\n",
    "- Use the trees where it is OOB to predict its response.\n",
    "- **Regression:** Average the predictions from these trees.\n",
    "- **Classification:** Take a majority vote across these trees.\n",
    "- This gives an OOB prediction for each observation.\n",
    "\n",
    "### OOB Error:\n",
    "- Compute the error (e.g., MSE for regression, misclassification rate for classification) between the OOB predictions and the true responses for all (n) observations.\n",
    "- The OOB error is a valid estimate of the test error because it uses predictions from trees that didn't see the observation during training, mimicking a test set.\n",
    "- **Advantage:** OOB error is computationally efficient, especially for large datasets where cross-validation (e.g., leave-one-out) is slow.\n",
    "- **Equivalence:** With a large (B), OOB error is nearly equivalent to leave-one-out cross-validation error.\n",
    "\n",
    "### Heart Dataset Example:\n",
    "In Figure 8.8, the green and blue curves show the OOB error for the Heart dataset, which is lower than the test error (black and orange curves) \"by chance.\" This suggests bagging performs well, and the OOB error is a reliable estimate of test performance.\n",
    "\n",
    "## Variable Importance Measures\n",
    "\n",
    "Bagging improves accuracy but sacrifices interpretability, as combining (B) trees (e.g., 100) makes it impossible to visualize a single tree like Figure 8.1 (Hitters) or 8.6 (Heart). To address this, bagging provides variable importance measures to identify which predictors are most influential.\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "#### Regression Trees:\n",
    "- For each predictor, compute the total reduction in RSS (from Section 8.1.1) due to splits on that predictor, averaged over all (B) trees.\n",
    "- A large average RSS reduction indicates an important predictor.\n",
    "\n",
    "#### Classification Trees:\n",
    "- For each predictor, compute the total reduction in Gini index (from Section 8.1.2) due to splits on that predictor, averaged over all (B) trees.\n",
    "- A large average Gini reduction indicates an important predictor.\n",
    "\n",
    "### Heart Dataset Example:\n",
    "\n",
    "**Figure 8.9:** Shows a variable importance plot for the Heart dataset.\n",
    "- Importance is measured as the mean decrease in Gini index for each predictor, expressed relative to the maximum.\n",
    "- **Top Predictors:** Thal, Ca (number of major vessels colored by fluoroscopy), and ChestPain have the largest mean decrease in Gini index, indicating they are the most important for predicting heart disease.\n",
    "- **Interpretation:** These variables frequently appear in splits across the (B) trees and significantly improve node purity, making them critical to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5300bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6d8a274",
   "metadata": {},
   "source": [
    "## What are Random Forests?\n",
    "\n",
    "Random forests are an ensemble method that builds on bagging (Section 8.2.1) by adding a key modification to reduce the correlation between decision trees, thereby further reducing variance and improving predictive accuracy. Like bagging, random forests combine predictions from multiple decision trees trained on bootstrap samples, but they introduce randomness in the splitting process to make the trees less similar.\n",
    "\n",
    "### How Random Forests Work\n",
    "\n",
    "Random forests are similar to bagging but include a \"small tweak\" to decorrelate the trees:\n",
    "\n",
    "#### Bootstrap Sampling (Same as Bagging):\n",
    "- Generate (B) bootstrap samples from the training dataset by sampling (n) observations with replacement (as in bagging).\n",
    "- Each bootstrap sample is used to train a decision tree.\n",
    "\n",
    "#### Random Predictor Selection:\n",
    "- When building each tree, at each split, instead of considering all (p) predictors (features) to find the best split, randomly select a subset of (m) predictors.\n",
    "- The split can only use one of these (m) predictors (chosen to minimize the splitting criterion, e.g., Gini index for classification, RSS for regression).\n",
    "- A fresh random sample of (m) predictors is chosen at each split, ensuring variability across splits.\n",
    "\n",
    "**Typical Choice of (m):** The book suggests m ≈ √p, where (p) is the total number of predictors. For example:\n",
    "- In the Heart dataset (p = 13), m ≈ √13 ≈ 4.\n",
    "\n",
    "#### Aggregate Predictions:\n",
    "\n",
    "##### Regression: \n",
    "Average the predictions from all (B) trees:\n",
    "```\n",
    "f̂_rf(x) = (1/B) ∑ᵦ₌₁ᴮ f̂*b(x)\n",
    "```\n",
    "where f̂*b(x) is the prediction from the (b)-th tree.\n",
    "\n",
    "##### Classification: \n",
    "Take a majority vote across the (B) trees to predict the most common class.\n",
    "\n",
    "**Example:** For the Heart dataset, predict \"Yes\" or \"No\" for heart disease by majority vote across (B) trees.\n",
    "\n",
    "### Why Decorrelate Trees?\n",
    "\n",
    "The key innovation of random forests is decorrelating the trees to achieve a greater reduction in variance compared to bagging. Here's why this matters:\n",
    "\n",
    "#### Problem with Bagging:\n",
    "- In bagging, each tree is trained on a bootstrap sample, but all (p) predictors are considered at every split.\n",
    "- If there's a very strong predictor (e.g., Thal in the Heart dataset), most or all bagged trees will use it for the top split (e.g., Thal = normal vs. others).\n",
    "- This makes the bagged trees highly correlated, as they have similar structures (e.g., similar top splits).\n",
    "- Averaging highly correlated predictions doesn't reduce variance as effectively as averaging uncorrelated predictions.\n",
    "\n",
    "#### Random Forest Solution:\n",
    "- By randomly selecting m < p predictors at each split, random forests ensure that the strong predictor is not always available.\n",
    "- On average, (p - m)/p of the splits won't consider the strong predictor, giving other predictors (e.g., Ca, ChestPain) a chance to be used.\n",
    "- This decorrelates the trees, making them more diverse. Averaging less correlated trees leads to a greater reduction in variance, improving test accuracy.\n",
    "\n",
    "#### Intuition:\n",
    "- Imagine a dataset where Thal is a dominant predictor for heart disease. In bagging, most trees might split on Thal first, making them similar. In random forests, some splits might use Age, Ca, or ChestPain instead, creating varied trees.\n",
    "- Combining these diverse trees reduces the overall variance, as their errors are less likely to align.\n",
    "\n",
    "### Key Parameter: (m) (Number of Predictors per Split)\n",
    "\n",
    "**Definition:** (m) is the number of predictors randomly sampled at each split.\n",
    "\n",
    "**Typical Value:** m ≈ √p (e.g., m ≈ 4 for p = 13 in the Heart dataset).\n",
    "\n",
    "**Special Case:**\n",
    "- If m = p, random forests reduce to bagging, as all predictors are considered at each split, losing the decorrelation benefit.\n",
    "\n",
    "**Choosing (m):**\n",
    "- **Smaller (m)** (e.g., √p) is effective when predictors are highly correlated, as it forces diversity by limiting the influence of dominant predictors.\n",
    "- **Larger (m)** (closer to (p)) is closer to bagging and may be better when predictors are less correlated.\n",
    "\n",
    "### Application to the Heart Dataset\n",
    "\n",
    "**Context:** Random forests are applied to the Heart dataset (Section 8.1.2) to predict heart disease (Yes/No) using 13 predictors (e.g., Thal, Ca, ChestPain).\n",
    "\n",
    "**Figure 8.8:**\n",
    "- Shows the test error and OOB error (out-of-bag error, from Section 8.2.1) for random forests with m = √p ≈ 4.\n",
    "- Random forests achieve a lower test error and OOB error compared to bagging (m = p), demonstrating the benefit of decorrelating trees.\n",
    "- The dashed line (single tree error) is higher, confirming that both bagging and random forests outperform a single classification tree.\n",
    "\n",
    "**Result:** Using m = √p reduces error compared to bagging, as it forces trees to use a variety of predictors, not just the strongest ones like Thal.\n",
    "\n",
    "### Application to Biological Dataset\n",
    "\n",
    "**Context:** A high-dimensional dataset with 4,718 gene expression measurements from 349 patients, labeled as normal or one of 14 cancer types (15 classes total).\n",
    "\n",
    "**Setup:**\n",
    "- Select the 500 genes with the highest variance in the training set.\n",
    "- Split data into training and test sets.\n",
    "- Apply random forests with different values of (m).\n",
    "\n",
    "**Results (Figure 8.10):**\n",
    "- A single classification tree has a high error rate of 45.7%.\n",
    "- The null rate (predicting the most common class) is 75.4%, indicating a challenging problem.\n",
    "- Random forests with 400 trees achieve good performance, and m = √p (e.g., √500 ≈ 22) slightly outperforms bagging (m = p = 500).\n",
    "\n",
    "**Interpretation:** The decorrelation from using m = √p helps when many predictors (genes) are correlated, improving classification accuracy over bagging.\n",
    "\n",
    "### Key Features of Random Forests\n",
    "\n",
    "#### No Overfitting with Large (B):\n",
    "- Like bagging, increasing the number of trees (B) (e.g., 400 or 1000) doesn't lead to overfitting. Choose a (B) large enough for the error to stabilize (as seen in Figure 8.10).\n",
    "\n",
    "#### OOB Error:\n",
    "- Random forests use OOB error (Section 8.2.1) to estimate test error without cross-validation, as each tree uses about two-thirds of the data, leaving one-third OOB.\n",
    "- In the Heart dataset, OOB error tracks test error well (Figure 8.8).\n",
    "\n",
    "#### Variable Importance:\n",
    "- Like bagging, random forests provide variable importance measures (e.g., mean decrease in Gini index for classification), as shown in Figure 8.9 for the Heart dataset (Thal, Ca, ChestPain are key).\n",
    "- In the biological dataset, importance measures could identify genes most predictive of cancer types.\n",
    "\n",
    "### Comparison to Bagging\n",
    "\n",
    "**Bagging:** Uses all (p) predictors at each split, leading to correlated trees if a strong predictor dominates (e.g., Thal in Heart).\n",
    "\n",
    "**Random Forests:** Uses m < p predictors (e.g., m = √p) at each split, decorrelating trees and reducing variance further.\n",
    "\n",
    "**Heart Dataset:** Random forests (m = √p) outperform bagging (m = p) in Figure 8.8, showing lower test and OOB errors.\n",
    "\n",
    "**When to Use Smaller (m):** Best for datasets with many correlated predictors (e.g., gene expressions in the biological dataset), as it ensures diverse trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36c8da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "460d241b",
   "metadata": {},
   "source": [
    "## What is Boosting?\n",
    "\n",
    "Boosting is an ensemble method that combines multiple decision trees (weak learners) to create a strong predictive model, similar to bagging and random forests (Sections 8.2.1 and 8.2.2). However, unlike bagging and random forests, which build trees independently, boosting grows trees sequentially, where each tree learns from the mistakes (residuals) of the previous trees. This sequential approach makes boosting a powerful method for improving accuracy, particularly for regression and classification tasks.\n",
    "\n",
    "### How Boosting Works\n",
    "\n",
    "Boosting builds a model by iteratively fitting small decision trees to the residuals (errors) of the current model, gradually improving predictions. The process is described in Algorithm 8.2 for regression trees, with a note that classification boosting is similar but more complex (details omitted in the book).\n",
    "\n",
    "#### Algorithm 8.2: Boosting for Regression Trees\n",
    "\n",
    "**Initialize:**\n",
    "- Set the initial model prediction to zero: $\\hat{f}(x) = 0$\n",
    "- Set the residuals for each observation to the actual response: $r_i = y_i$ (e.g., for the Hitters dataset, $y_i$ is log(Salary))\n",
    "\n",
    "**For $b = 1, 2, \\ldots, B$ (iterate over $B$ trees):**\n",
    "\n",
    "**(a) Fit a Small Tree:**\n",
    "- Fit a decision tree $\\hat{f}^b(x)$ with $d$ splits ($d + 1$ terminal nodes) to the current residuals $(X, r)$, treating the residuals as the response variable\n",
    "- Example: For Hitters, fit a tree to the residuals (differences between observed and predicted log(Salary))\n",
    "\n",
    "**(b) Update the Model:**\n",
    "- Add a shrunken version of the new tree to the model:\n",
    "  $$\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)$$\n",
    "  where $\\lambda$ is a shrinkage parameter (e.g., 0.01 or 0.001) that controls the contribution of each tree\n",
    "\n",
    "**(c) Update the Residuals:**\n",
    "- Update the residuals by subtracting the shrunken predictions:\n",
    "  $$r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x_i)$$\n",
    "  This focuses the next tree on the remaining errors\n",
    "\n",
    "**Output the Boosted Model:**\n",
    "The final model is the sum of all shrunken trees:\n",
    "$$\\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x)$$\n",
    "\n",
    "#### Key Idea: Learn Slowly\n",
    "\n",
    "Unlike a single large tree that fits the data aggressively (risking overfitting), boosting learns slowly by:\n",
    "- Fitting small trees (with few splits, e.g., $d = 1$) to residuals\n",
    "- Using a small $\\lambda$ to add each tree's contribution gradually\n",
    "\n",
    "This slow learning reduces overfitting and allows the model to focus on areas where predictions are poor, improving overall performance.\n",
    "\n",
    "### Boosting vs. Bagging and Random Forests\n",
    "\n",
    "#### Bagging (Section 8.2.1):\n",
    "- Builds $B$ independent trees on bootstrap samples\n",
    "- Aggregates predictions by averaging (regression) or majority vote (classification)\n",
    "- Reduces variance but doesn't adjust for previous trees' errors\n",
    "- Uses deep, unpruned trees\n",
    "\n",
    "#### Random Forests (Section 8.2.2):\n",
    "- Builds $B$ trees on bootstrap samples, with random predictor selection ($m \\approx \\sqrt{p}$) at each split to decorrelate trees\n",
    "- Also reduces variance but doesn't use sequential learning\n",
    "- Uses deep, unpruned trees\n",
    "\n",
    "#### Boosting:\n",
    "- Builds trees sequentially, with each tree fitting the residuals of the current model\n",
    "- Does not use bootstrap sampling; instead, modifies the dataset by updating residuals\n",
    "- Uses small trees (e.g., $d = 1$ or 2 splits) and a shrinkage parameter $\\lambda$ to control learning rate\n",
    "- Can overfit if $B$ is too large, unlike bagging and random forests\n",
    "\n",
    "### Tuning Parameters in Boosting\n",
    "\n",
    "Boosting has three key parameters that control its performance:\n",
    "\n",
    "#### 1. Number of Trees ($B$):\n",
    "- The total number of trees in the ensemble\n",
    "- Unlike bagging and random forests, boosting can overfit if $B$ is too large, but overfitting occurs slowly\n",
    "- Use cross-validation to select an optimal $B$\n",
    "\n",
    "#### 2. Shrinkage Parameter ($\\lambda$):\n",
    "- A small positive number (e.g., 0.01 or 0.001) that scales the contribution of each tree\n",
    "- Smaller $\\lambda$ slows learning, requiring a larger $B$ for good performance but reducing overfitting\n",
    "- The choice of $\\lambda$ depends on the problem (e.g., complex datasets may need smaller $\\lambda$)\n",
    "\n",
    "#### 3. Interaction Depth ($d$):\n",
    "- The number of splits in each tree, controlling tree complexity ($d + 1$ terminal nodes)\n",
    "- $d = 1$: Each tree is a stump (single split), modeling an additive model (each tree involves only one predictor)\n",
    "- Larger $d$: Allows interactions between up to $d$ predictors (e.g., $d = 2$ can model interactions like Years and Hits in Hitters)\n",
    "- Smaller $d$ (e.g., 1 or 2) often works well and improves interpretability\n",
    "\n",
    "### Application to the 15-Class Cancer Gene Expression Dataset\n",
    "\n",
    "#### Context:\n",
    "The dataset (introduced in Section 8.2.2) includes 4,718 gene expression measurements for 349 patients, with a qualitative response of 15 classes (normal or one of 14 cancer types). Random forests were applied to the 500 genes with the highest variance.\n",
    "\n",
    "#### Boosting Application:\n",
    "- Boosting is used to classify cancer type vs. normal\n",
    "- Trees are grown sequentially, fitting residuals with a shrinkage parameter $\\lambda = 0.01$\n",
    "- Two interaction depths are tested: $d = 1$ (stumps) and $d = 2$\n",
    "\n",
    "#### Results (Figure 8.11):\n",
    "- **Test Error**: Plotted as a function of the number of trees ($B$)\n",
    "- **Depth-1 Trees (Stumps)**: Perform well with enough trees, slightly outperforming depth-2 trees\n",
    "- **Comparison to Random Forests**: Both boosted models ($d = 1, 2$) outperform random forests, though standard errors (~0.02) suggest the differences may not be significant\n",
    "- **Single Tree**: Has a test error of 24%, much higher than boosted models\n",
    "\n",
    "### Interpretation:\n",
    "- Stumps ($d = 1$) model additive effects (each tree uses one gene), which is sufficient for this dataset and more interpretable\n",
    "- Smaller trees ($d = 1$) work well because boosting accounts for previous trees, unlike random forests, which use deep trees\n",
    "- Boosting's sequential learning focuses on hard-to-predict cases, improving accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486eb7b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "093206c5",
   "metadata": {},
   "source": [
    "## What are Bayesian Additive Regression Trees (BART)?\n",
    "\n",
    "Bayesian Additive Regression Trees (BART) is an ensemble method that uses decision trees as building blocks to predict a continuous response (regression), with a focus on this section (classification is not covered). BART combines elements of bagging, random forests, and boosting but introduces a unique Bayesian approach to generate trees, balancing fit and complexity to avoid overfitting. It's designed to provide high predictive accuracy with minimal tuning.\n",
    "\n",
    "### How BART Works\n",
    "\n",
    "BART builds a model by combining predictions from $K$ regression trees across $B$ iterations, with each tree contributing to the overall prediction. Unlike bagging and random forests, which average independent trees, or boosting, which sequentially fits trees to residuals, BART uses a Bayesian framework to iteratively perturb trees to fit partial residuals, incorporating randomness to enhance robustness.\n",
    "\n",
    "#### Key Notation\n",
    "\n",
    "- $K$: Number of trees in each iteration (e.g., $K = 200$)\n",
    "- $B$: Number of iterations (e.g., $B = 1,000$)\n",
    "- $\\hat{f}_{kb}(x)$: Prediction at input $x$ for the $k$-th tree in the $b$-th iteration\n",
    "- $\\hat{f}_b(x) = \\sum_{k=1}^K \\hat{f}_{kb}(x)$: Total prediction in the $b$-th iteration, summing the $K$ trees\n",
    "- $r_i$: Partial residual for observation $i$, used to update trees\n",
    "\n",
    "### Algorithm 8.3: Bayesian Additive Regression Trees\n",
    "\n",
    "#### Initialize:\n",
    "Set each of the $K$ trees in the first iteration ($b = 1$) to a single root node with a constant prediction:\n",
    "$$\\hat{f}_{k1}(x) = \\frac{1}{nK} \\sum_{i=1}^n y_i$$\n",
    "\n",
    "where $\\sum_{i=1}^n y_i / n$ is the mean response (e.g., mean log(Salary) for Hitters).\n",
    "\n",
    "The initial model prediction is:\n",
    "$$\\hat{f}_1(x) = \\sum_{k=1}^K \\hat{f}_{k1}(x) = \\frac{1}{n} \\sum_{i=1}^n y_i$$\n",
    "(the overall mean response).\n",
    "\n",
    "#### For $b = 2, \\ldots, B$ (iterate over $B$ iterations):\n",
    "\n",
    "**For each tree $k = 1, \\ldots, K$:**\n",
    "\n",
    "**(i) Compute Partial Residuals:**\n",
    "For each observation $i$, calculate the partial residual by subtracting the predictions of all other trees (from the current and previous iterations):\n",
    "$$r_i = y_i - \\sum_{k' < k} \\hat{f}_{k'b}(x_i) - \\sum_{k' > k} \\hat{f}_{k',b-1}(x_i)$$\n",
    "\n",
    "This represents the error not explained by the other $K-1$ trees.\n",
    "\n",
    "**(ii) Update the $k$-th Tree:**\n",
    "Instead of fitting a new tree from scratch, randomly perturb the $k$-th tree from the previous iteration ($\\hat{f}_{k,b-1}(x)$) to better fit the partial residuals $r_i$.\n",
    "\n",
    "**Perturbations (see Figure 8.12):**\n",
    "- **Change tree structure**: Add or prune branches (e.g., add a split like Years < 4.5 or remove one)\n",
    "- **Change terminal node predictions**: Adjust the predicted values in the leaves\n",
    "- Perturbations that improve the fit to the partial residuals are favored (guided by a Bayesian posterior distribution, though details are omitted)\n",
    "\n",
    "#### Compute the Iteration's Prediction:\n",
    "$$\\hat{f}_b(x) = \\sum_{k=1}^K \\hat{f}_{kb}(x)$$\n",
    "\n",
    "#### Output the Final Model:\n",
    "- Discard the first $L$ iterations (the burn-in period, e.g., $L = 200$) to exclude early, less accurate models\n",
    "- Average the predictions from the remaining iterations:\n",
    "$$\\hat{f}(x) = \\frac{1}{B - L} \\sum_{b=L+1}^B \\hat{f}_b(x)$$\n",
    "\n",
    "Optionally, use the distribution of $\\hat{f}_{L+1}(x), \\ldots, \\hat{f}_B(x)$ to compute uncertainty (e.g., percentiles).\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Random Perturbations**: Instead of fitting new trees, BART perturbs existing trees (e.g., adding/pruning branches, adjusting predictions), limiting overfitting (Figure 8.12)\n",
    "- **Small Trees**: BART uses small trees (like boosting) to avoid overfitting, unlike the deep trees in bagging and random forests\n",
    "- **Bayesian Approach**: Tree perturbations are drawn from a posterior distribution, treating the model as a Bayesian inference problem (details beyond the book's scope)\n",
    "- **Burn-In Period**: Early iterations ($b = 1, \\ldots, L$) are discarded because they may not fit well as the model is still adjusting\n",
    "\n",
    "### BART vs. Bagging, Random Forests, and Boosting\n",
    "\n",
    "#### Bagging (Section 8.2.1):\n",
    "- Builds independent deep trees on bootstrap samples, averaging predictions\n",
    "- No sequential learning; uses all predictors per split\n",
    "\n",
    "#### Random Forests (Section 8.2.2):\n",
    "- Builds independent deep trees with random predictor subsets ($m \\approx \\sqrt{p}$) to decorrelate trees\n",
    "- No sequential learning; focuses on variance reduction\n",
    "\n",
    "#### Boosting (Section 8.2.3):\n",
    "- Sequentially fits small trees to residuals, using a shrinkage parameter $\\lambda$\n",
    "- No randomness in tree construction; can overfit with large $B$\n",
    "\n",
    "#### BART:\n",
    "- Combines randomness (like bagging/random forests) and sequential learning (like boosting)\n",
    "- Fits $K$ trees per iteration, updating each to partial residuals via random perturbations\n",
    "- Uses a Bayesian framework to guide perturbations, reducing overfitting\n",
    "- Small trees and burn-in improve robustness\n",
    "\n",
    "### Application to the Heart Dataset\n",
    "\n",
    "#### Context:\n",
    "BART is applied to the Heart dataset (Section 8.1.2) to predict heart disease (Yes/No, though BART here focuses on regression, possibly on a related continuous outcome or misclassification error).\n",
    "\n",
    "#### Setup:\n",
    "- Uses $K = 200$ trees and up to $B = 10,000$ iterations\n",
    "\n",
    "#### Results (Figure 8.13):\n",
    "- **Training and Test Errors**: Both errors fluctuate during the initial burn-in period (first 100 iterations, shown in gray) but stabilize afterward\n",
    "- **Low Overfitting**: The small gap between training and test errors indicates BART avoids overfitting, thanks to random perturbations and small trees\n",
    "\n",
    "#### Comparison to Boosting:\n",
    "- Boosting's test error approaches BART's but increases after a few hundred iterations, indicating overfitting\n",
    "- Boosting's training error decreases continuously, showing it fits the training data too closely\n",
    "- BART maintains stable performance with less overfitting\n",
    "\n",
    "**Interpretation**: BART's Bayesian approach and random perturbations make it robust, achieving competitive accuracy with minimal tuning.\n",
    "\n",
    "### Tuning Parameters in BART\n",
    "\n",
    "- **Number of Trees ($K$)**: Typically large (e.g., $K = 200$) to capture complex patterns\n",
    "- **Number of Iterations ($B$)**: Large (e.g., $B = 1,000$) to ensure convergence, but only iterations after the burn-in ($L$) are used\n",
    "- **Burn-In Period ($L$)**: Moderate (e.g., $L = 100$) to discard early, less accurate models\n",
    "- **Default Performance**: BART performs well \"out-of-the-box\" with minimal tuning (e.g., $K = 200, B = 1,000, L = 100$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35394d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
